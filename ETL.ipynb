{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fda9e2-2441-4e77-bd9c-94f555514b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved AI facts → Output.csv (1047 rows)\n",
      "Saved tables → tables.csv (434 rows)\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, time, argparse\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pdfplumber, pandas as pd\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "PDF_DEFAULT               = \"amazon.pdf\"\n",
    "FACTS_CSV_DEFAULT         = \"Output.csv\"\n",
    "TABLES_CSV_DEFAULT        = \"tables.csv\"\n",
    "MODEL_DEFAULT             = \"qwen-turbo\"  \n",
    "BASE_URL_DEFAULT          = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "API_KEY_ENV_DEFAULT       = \"DASHSCOPE_API_KEY\"  \n",
    "TEMPERATURE_DEFAULT       = 0.0\n",
    "MAX_CALLS_DEFAULT         = None\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a structured information extractor for financial/business PDFs. \"\n",
    "    \"Return STRICT JSON only (no prose). Extract quantitative facts and attach a short, grounded comment.\\n\"\n",
    ")\n",
    "\n",
    "USER_TMPL = \"\"\"Extract facts as a JSON array following the schema below.\n",
    "\n",
    "Rules:\n",
    "- Focus on quantitative facts (numbers/%, currency) and their meaning (metric). Add period if present.\n",
    "- currency_symbol: \"$\"|\"€\"|\"£\"|null. unit: \"USD\"|\"EUR\"|\"GBP\"|\"UNSPEC\".\n",
    "- Percent: set is_percent=true and value as numeric 12 for \"12%\".\n",
    "- Convert 'million'/'billion'/'trillion' to absolute base units in 'value' (e.g., \"$12 billion\" → 12000000000).\n",
    "- topic: one of [\"revenue\",\"operating_income\",\"net_income\",\"eps\",\"aws\",\"ads\",\"fcf\",\"margin\",\"guidance\",\"costs\",\"capex\",\"general\"].\n",
    "- evidence: short phrase (<=140 chars) anchoring the metric; comment (<=200 chars) — concise, non-speculative.\n",
    "- Always include the integer 'page' field.\n",
    "\n",
    "Example:\n",
    "[\n",
    "  {{\n",
    "    \"metric\": \"AWS revenue\",\n",
    "    \"value\": 90700000000,\n",
    "    \"unit\": \"USD\",\n",
    "    \"currency_symbol\": \"$\",\n",
    "    \"is_percent\": false,\n",
    "    \"topic\": \"aws\",\n",
    "    \"period\": \"FY2024\",\n",
    "    \"comment\": \"Revenue grew on migration of enterprise workloads to cloud\",\n",
    "    \"evidence\": \"AWS revenue increased ...\",\n",
    "    \"page\": {page}\n",
    "  }}\n",
    "]\n",
    "\n",
    "TEXT (page {page}):\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    text = \" \".join(text.split())\n",
    "    parts = re.split(r\"(?<=[\\.\\!\\?])\\s+(?=[A-Z0-9\\(])\", text)\n",
    "    return [p.strip() for p in parts if p.strip()] or ([text] if text.strip() else [])\n",
    "\n",
    "def chunkify(sents: List[str], max_sents: int = 10) -> List[str]:\n",
    "    return [\" \".join(sents[i:i+max_sents]) for i in range(0, len(sents), max_sents)]\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    _HAS_NEW_SDK = True\n",
    "except Exception:\n",
    "    import openai as openai_legacy\n",
    "    _HAS_NEW_SDK = False\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, api_key_env: str, model: str, base_url: Optional[str]):\n",
    "        api_key = os.getenv(api_key_env)\n",
    "        if not api_key:\n",
    "            raise RuntimeError(f\"Env var {api_key_env} not set\")\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "\n",
    "        if _HAS_NEW_SDK:\n",
    "            self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        else:\n",
    "            openai_legacy.api_key = api_key\n",
    "            if base_url:\n",
    "                openai_legacy.base_url = base_url\n",
    "            self.client = openai_legacy\n",
    "\n",
    "    @retry(\n",
    "        retry=retry_if_exception_type(Exception),\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=20),\n",
    "        stop=stop_after_attempt(5),\n",
    "        reraise=True\n",
    "    )\n",
    "    def extract(self, *, page: int, chunk: str, temperature: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        user = USER_TMPL.format(page=page, chunk=chunk)\n",
    "        if _HAS_NEW_SDK:\n",
    "            resp = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "                    {\"role\":\"user\",\"content\":user}\n",
    "                ],\n",
    "                response_format={\"type\":\"json_object\"}\n",
    "            )\n",
    "            txt = resp.choices[0].message.content\n",
    "        else:\n",
    "            resp = self.client.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "                    {\"role\":\"user\",\"content\":user}\n",
    "                ]\n",
    "            )\n",
    "            txt = resp.choices[0].message[\"content\"]\n",
    "\n",
    "        def coerce_list(obj):\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "            if isinstance(obj, dict) and \"items\" in obj:\n",
    "                return obj[\"items\"]\n",
    "            if isinstance(obj, dict):\n",
    "                return [obj]\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "        except Exception:\n",
    "            s, e = txt.find(\"[\"), txt.rfind(\"]\")\n",
    "            data = json.loads(txt[s:e+1]) if s != -1 and e != -1 and e > s else []\n",
    "\n",
    "        data = coerce_list(data)\n",
    "        cleaned: List[Dict[str, Any]] = []\n",
    "        for it in data:\n",
    "            if not isinstance(it, dict):\n",
    "                continue\n",
    "\n",
    "            it.setdefault(\"metric\", None)\n",
    "            it.setdefault(\"value\", None)\n",
    "            it.setdefault(\"unit\", None)\n",
    "            it.setdefault(\"currency_symbol\", None)\n",
    "            it.setdefault(\"is_percent\", False)\n",
    "            it.setdefault(\"topic\", \"general\")\n",
    "            it.setdefault(\"period\", None)\n",
    "            it.setdefault(\"comment\", None)\n",
    "            it.setdefault(\"evidence\", None)\n",
    "            it.setdefault(\"page\", page)\n",
    "\n",
    "            # unit normalization\n",
    "            unit = (it.get(\"unit\") or \"\").upper()\n",
    "            if unit not in {\"USD\",\"EUR\",\"GBP\",\"UNSPEC\"}:\n",
    "                sym = it.get(\"currency_symbol\")\n",
    "                unit = \"USD\" if sym == \"$\" else \"EUR\" if sym == \"€\" else \"GBP\" if sym == \"£\" else \"UNSPEC\"\n",
    "            it[\"unit\"] = unit\n",
    "\n",
    "            # value → float\n",
    "            val = it.get(\"value\")\n",
    "            try:\n",
    "                val = float(val) if val is not None else None\n",
    "            except Exception:\n",
    "                val = None\n",
    "            it[\"value\"] = val\n",
    "\n",
    "            # Truncate\n",
    "            if it[\"comment\"]:\n",
    "                it[\"comment\"] = str(it[\"comment\"])[:200]\n",
    "            if it[\"evidence\"]:\n",
    "                it[\"evidence\"] = str(it[\"evidence\"])[:140]\n",
    "\n",
    "            cleaned.append(it)\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "_SCALE_PATTERN = re.compile(r\"\\b(trillion|billion|million|tn|bn|mn)\\b\", re.I)\n",
    "\n",
    "def normalize_value_scale(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    try:\n",
    "        val = row.get(\"value\", None)\n",
    "        if val is None or row.get(\"is_percent\", False):\n",
    "            return row\n",
    "        unit = (row.get(\"unit\") or \"UNSPEC\").upper()\n",
    "        sym  = (row.get(\"currency_symbol\") or \"\")\n",
    "        if unit == \"UNSPEC\" and sym not in {\"$\", \"€\", \"£\"}:\n",
    "            return row\n",
    "\n",
    "        hint_text = \" \".join([str(row.get(\"evidence\") or \"\"), str(row.get(\"comment\") or \"\")])\n",
    "        m = _SCALE_PATTERN.search(hint_text)\n",
    "        if not m:\n",
    "            return row\n",
    "\n",
    "        scale = m.group(1).lower()\n",
    "        mult = 1.0\n",
    "        if scale in (\"million\", \"mn\"):\n",
    "            mult = 1e6\n",
    "        elif scale in (\"billion\", \"bn\"):\n",
    "            mult = 1e9\n",
    "        elif scale in (\"trillion\", \"tn\"):\n",
    "            mult = 1e12\n",
    "\n",
    "        if val is not None and val < 1e7:\n",
    "            row[\"value\"] = float(val) * mult\n",
    "        return row\n",
    "    except Exception:\n",
    "        return row\n",
    "\n",
    "def parse_pages_arg(pages_str: str, total_pages: int) -> List[int]:\n",
    "    if not pages_str:\n",
    "        return list(range(1, total_pages + 1))\n",
    "    out: List[int] = []\n",
    "    for part in pages_str.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if \"-\" in part:\n",
    "            a, b = part.split(\"-\", 1)\n",
    "            try:\n",
    "                a, b = int(a), int(b)\n",
    "                if a <= b:\n",
    "                    out.extend(range(a, b + 1))\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                out.append(int(part))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return [p for p in out if 1 <= p <= total_pages]\n",
    "\n",
    "def ai_extract_pdf_to_rows(\n",
    "    pdf_path: str,\n",
    "    model: str = MODEL_DEFAULT,\n",
    "    api_key_env: str = API_KEY_ENV_DEFAULT,\n",
    "    base_url: Optional[str] = BASE_URL_DEFAULT,\n",
    "    temperature: float = TEMPERATURE_DEFAULT,\n",
    "    max_calls: Optional[int] = MAX_CALLS_DEFAULT,\n",
    "    pages: Optional[str] = None,\n",
    "    topics_filter: Optional[List[str]] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    client = LLMClient(api_key_env=api_key_env, model=model, base_url=base_url)\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        allowed_pages = parse_pages_arg(pages or \"\", len(pdf.pages))\n",
    "        calls = 0\n",
    "        for page_idx, page in enumerate(pdf.pages, start=1):\n",
    "            if allowed_pages and page_idx not in allowed_pages:\n",
    "                continue\n",
    "\n",
    "            txt = page.extract_text() or \"\"\n",
    "            if not txt.strip():\n",
    "                continue\n",
    "\n",
    "            sents  = split_sentences(txt)\n",
    "            chunks = chunkify(sents, max_sents=10)\n",
    "\n",
    "            for ch in chunks:\n",
    "                items = client.extract(page=page_idx, chunk=ch, temperature=temperature)\n",
    "\n",
    "                for it in items:\n",
    "                    it[\"pdf_file\"] = os.path.basename(pdf_path)\n",
    "\n",
    "                items = [normalize_value_scale(it) for it in items]\n",
    "\n",
    "                if topics_filter:\n",
    "                    topics_set = {t.strip().lower() for t in topics_filter}\n",
    "                    items = [it for it in items if str(it.get(\"topic\",\"general\")).lower() in topics_set]\n",
    "\n",
    "                rows.extend(items)\n",
    "                calls += 1\n",
    "                if max_calls and calls >= max_calls:\n",
    "                    return rows\n",
    "                time.sleep(0.10)\n",
    "    return rows\n",
    "\n",
    "def extract_tables_simple(pdf_path: str) -> pd.DataFrame:\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_idx, page in enumerate(pdf.pages, start=1):\n",
    "            for tbl in page.extract_tables() or []:\n",
    "                df = pd.DataFrame(tbl)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                header_row = df.iloc[0].tolist()\n",
    "                non_null = sum(1 for x in header_row if pd.notnull(x) and str(x).strip() != \"\")\n",
    "                unique   = len(set(map(lambda x: str(x).strip(), header_row)))\n",
    "                if non_null >= max(2, int(0.6 * len(header_row))) and unique == len(header_row):\n",
    "                    df.columns = [f\"{str(c).strip()}\" if str(c).strip() else f\"col_{i}\" for i, c in enumerate(header_row)]\n",
    "                    df = df.drop(index=0).reset_index(drop=True)\n",
    "                else:\n",
    "                    df.columns = [f\"col_{i}\" for i in range(df.shape[1])]\n",
    "\n",
    "                df.insert(0, \"page\", page_idx)\n",
    "                frames.append(df)\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def run_notebook_defaults():\n",
    "    facts = ai_extract_pdf_to_rows(\n",
    "        pdf_path=PDF_DEFAULT,\n",
    "        model=MODEL_DEFAULT,\n",
    "        api_key_env=API_KEY_ENV_DEFAULT,\n",
    "        base_url=BASE_URL_DEFAULT,\n",
    "        temperature=TEMPERATURE_DEFAULT,\n",
    "        max_calls=MAX_CALLS_DEFAULT\n",
    "    )\n",
    "    df = pd.DataFrame(facts, columns=[\n",
    "        \"pdf_file\",\"page\",\"metric\",\"value\",\"unit\",\"currency_symbol\",\n",
    "        \"is_percent\",\"topic\",\"period\",\"comment\",\"evidence\"\n",
    "    ])\n",
    "    df.to_csv(FACTS_CSV_DEFAULT, index=False)\n",
    "    print(f\"Saved AI facts → {FACTS_CSV_DEFAULT} ({len(df)} rows)\")\n",
    "\n",
    "    tdf = extract_tables_simple(PDF_DEFAULT)\n",
    "    if not tdf.empty:\n",
    "        tdf.to_csv(TABLES_CSV_DEFAULT, index=False)\n",
    "        print(f\"Saved tables → {TABLES_CSV_DEFAULT} ({len(tdf)} rows)\")\n",
    "    else:\n",
    "        print(\"ℹ️ No tables detected by pdfplumber. For complex tables consider camelot/tabula.\")\n",
    "\n",
    "def run_cli():\n",
    "    ap = argparse.ArgumentParser(description=\"AI-enhanced ETL: PDF → facts.csv + tables.csv (Qwen Turbo via DashScope)\")\n",
    "    ap.add_argument(\"--pdf\", default=PDF_DEFAULT, help=\"Path to PDF\")\n",
    "    ap.add_argument(\"--facts_csv\", default=FACTS_CSV_DEFAULT, help=\"Output CSV for AI facts\")\n",
    "    ap.add_argument(\"--tables_csv\", default=TABLES_CSV_DEFAULT, help=\"Output CSV for tables\")\n",
    "    ap.add_argument(\"--model\", default=MODEL_DEFAULT, help=\"LLM model (default: qwen-turbo)\")\n",
    "    ap.add_argument(\"--api_key_env\", default=API_KEY_ENV_DEFAULT, help=\"Env var for API key (default: DASHSCOPE_API_KEY)\")\n",
    "    ap.add_argument(\"--base_url\", default=BASE_URL_DEFAULT, help=\"Base URL (DashScope compatible mode)\")\n",
    "    ap.add_argument(\"--temperature\", type=float, default=TEMPERATURE_DEFAULT)\n",
    "    ap.add_argument(\"--max_calls\", type=int, default=None)\n",
    "    ap.add_argument(\"--pages\", type=str, default=None, help='Pages filter like \"1-5,7,10-12\"')\n",
    "    ap.add_argument(\"--topics\", type=str, default=None, help='Comma list of topics (e.g., \"aws,revenue,fcf\")')\n",
    "    ap.add_argument(\"--no_tables\", action=\"store_true\", help=\"Skip table extraction\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    topics_filter = [t.strip() for t in args.topics.split(\",\")] if args.topics else None\n",
    "\n",
    "    facts = ai_extract_pdf_to_rows(\n",
    "        pdf_path=args.pdf,\n",
    "        model=args.model,\n",
    "        api_key_env=args.api_key_env,\n",
    "        base_url=args.base_url,\n",
    "        temperature=args.temperature,\n",
    "        max_calls=args.max_calls,\n",
    "        pages=args.pages,\n",
    "        topics_filter=topics_filter\n",
    "    )\n",
    "    df = pd.DataFrame(facts)\n",
    "    df.to_csv(args.facts_csv, index=False)\n",
    "    print(f\"Saved AI facts → {args.facts_csv} ({len(df)} rows)\")\n",
    "\n",
    "    if not args.no_tables:\n",
    "        tdf = extract_tables_simple(args.pdf)\n",
    "        if not tdf.empty:\n",
    "            tdf.to_csv(args.tables_csv, index=False)\n",
    "            print(f\"Saved tables → {args.tables_csv} ({len(tdf)} rows)\")\n",
    "        else:\n",
    "            print(\"No tables detected by pdfplumber. For complex tables consider camelot/tabula.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if any(\"ipykernel\" in a or \"jupyter\" in a for a in sys.argv):\n",
    "        run_notebook_defaults()\n",
    "    else:\n",
    "        run_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "942130d8-8c61-4816-849b-7242a5de6f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumberNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\arsen\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.3.33)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pdfplumber) (10.2.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (46.0.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (0.4.29)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-openai) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.104.2->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.8/5.6 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.6/5.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/2.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.8/2.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber pandas python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c75af6-d71f-47cf-8451-1e3c97079671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\arsen\\anaconda3\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.3.33)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\arsen\\anaconda3\\lib\\site-packages (0.3.76)\n",
      "Requirement already satisfied: openai in c:\\users\\arsen\\anaconda3\\lib\\site-packages (1.109.1)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pdfplumber) (10.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (46.0.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (0.4.29)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langchain-core) (2.11.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic>=2.7.4->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic>=2.7.4->langchain-core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from pydantic>=2.7.4->langchain-core) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core) (2.5.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arsen\\anaconda3\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber pandas langchain-openai langchain-core openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44e2000-e18d-488d-aabc-f99a81cd0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-2de649202adc42b6bff0b0a16865cb7e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789cf1b-4825-40d1-8bb7-eff4f31cb202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
